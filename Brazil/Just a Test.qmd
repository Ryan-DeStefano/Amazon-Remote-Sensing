---
title: "Test"
format: html
editor: visual
---

```{r}
lsat.dt <- do.call("rbind", lapply(c("Top_10_Brazil_20_points_each_plantation.csv"), fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=5, fig.height=5}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             doy.rng = 1:365, 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Function to exract sample points that came from the same polygon, will be used to help fit the mean evi values of a polygon rather than individual sample points. Will need to be updated, works for when there are 10 points sampled from each polygon
extract_group <- function(sample_id) {
  group_number <- as.numeric(sub("S_", "", sample_id))
  higher_order_group <- ((group_number - 1) %/% 20) + 1
  return(higher_order_group)
}

means <- lsat.dt %>%
  mutate(group.id = extract_group(sample.id))

grouped_means <- means %>%
  group_by(doy, year, group.id) %>%
  summarize(evi = mean(evi))

grouped_means <- filter(grouped_means, year >= 2010)

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- updated_curve_fit_mean(grouped_means, si = 'evi', spar = .6)
```
