---
title: "Brazil AOI Landsat Application"
author: "Ryan DeStefano"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
library(sf)
library(dplyr)
library(purrr)
library(data.table)
library(stringr)
library(rgee)
library(lwgeom)
library(leaflet)
library(LandsatTS)
library(ggplot2)
```

Sampling points from the entirety of the plantations (not specific to plantation)

```{r}
set.seed(27)

paracrop <- st_read(
  "C:/Users/Ryman/OneDrive/Documents/Thesis/ParaCropShapefiles (Master)/ParaCropShapefiles/ParaCropShapefiles.shp")

paracrop <- st_make_valid(paracrop)

# Check the current coordinate reference system
st_crs(paracrop)

# Transform to WGS84 (EPSG:4326)
paracrop <- st_transform(paracrop, crs = 4326)

# Sample points
n.pts <- 1
test.pts.sf <- st_sample(x = paracrop, size = n.pts) %>% st_sf()
test.pts.sf$sample_id <- paste0('S_', 1:n.pts)

# Creating the Leaflet map
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addPolygons(data = paracrop,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

Sorting top 10 rows based on area and sampling a certain number of points from each area (specific to plantation). USED THIS FOR Top_10_Brazil_1 and 2

```{r}
top_10_para <- paracrop %>% 
  arrange(desc(Shape_Area)) 

top_10_para$estimated_points <- top_10_para$Shape_Area / 0.00000006

top_10_para <- top_10_para[948,]

bottom_10_para <- paracrop %>% 
  arrange(Shape_Area) %>%
  slice_head(n = 10)

# Define the number of points to sample from each polygon
n.pts_per_polygon <- 1  # Change this value as needed

# Create an empty tibble to store the sampled points
test <- tibble(geometry = vector("list", nrow(top_10_para) * n.pts_per_polygon),
               sample_id = character(nrow(top_10_para) * n.pts_per_polygon))

# Initialize the sample ID counter
sample_counter <- 1

# Loop through each region to sample points
for (i in 1:nrow(top_10_para)) {
  # Sample points from the i-th polygon
  pts <- st_sample(x = top_10_para[i, ], size = n.pts_per_polygon)
  
  # Convert the sampled points to their WKT representation
  point_wkt <- st_as_text(pts)
  
  # Determine the indices to store the sampled points
  start_index <- (i - 1) * n.pts_per_polygon + 1
  end_index <- i * n.pts_per_polygon
  
  # Store the WKT representation of the sampled point geometries and sample IDs in the test tibble
  test$geometry[start_index:end_index] <- point_wkt
  test$sample_id[start_index:end_index] <- paste0('S_', sample_counter:(sample_counter + n.pts_per_polygon - 1))
        
  # Increment the sample ID counter
  sample_counter <- sample_counter + n.pts_per_polygon
}

# Convert the test tibble to an sf object
test_sf <- st_as_sf(test, wkt = "geometry", crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addCircleMarkers(data = test_sf, 
                   color = 'white',
                   opacity = 0.9,
                   fillColor = 'fuchsia',
                   fillOpacity = 0.75,
                   weight = 1,
                   radius = 5) %>%
  addPolygons(data = top_10_para,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

```{r}
# Create a vector specifying the number of points to be sampled from each polygon
n_pts_per_polygon <- rep(1, times=10)# Example vector, replace with your desired values

# Initialize the tibble with the appropriate size
test <- tibble(geometry = vector("list", sum(n_pts_per_polygon)),
               sample_id = character(sum(n_pts_per_polygon)))

# Initialize the sample ID counter
sample_counter <- 1

# Loop through each region to sample points
for (i in 1:10) {
  # Sample points from the i-th polygon
  pts <- st_sample(x = top_10_para[i, ], size = n_pts_per_polygon[i])
  
  # Convert the sampled points to their WKT representation
  point_wkt <- st_as_text(pts)
  
  # Determine the indices to store the sampled points
  start_index <- sample_counter
  end_index <- sample_counter + n_pts_per_polygon[i] - 1
  
  # Store the WKT representation of the sampled point geometries and sample IDs in the test tibble
  test$geometry[start_index:end_index] <- point_wkt
  test$sample_id[start_index:end_index] <- paste0('S_', start_index:end_index)
        
  # Increment the sample ID counter
  sample_counter <- end_index + 1
}

test_sf <- st_as_sf(test, wkt = "geometry", crs=4326)
```

Gets all pixel centers for all regions (will take too long, don't use, 1 million points)

```{r}
all_pixel_lists <- list()

for (i in 1){
  poly <- top_10_para$geometry[i]
  if (st_geometry_type(poly) == "MULTIPOLYGON"){
    polys <- st_cast(poly, "POLYGON")
    for (j in 1:length(polys)){
      paracrop_poly <- st_sfc(polys[j], crs = 4326) %>% st_sf()
      pixel_list_test_poly <- lsat_get_pixel_centers(paracrop_poly, plot_map=FALSE, pixel_prefix="S")
    
      all_pixel_lists[[i]] <- pixel_list_test_poly
    }
  }
  else{
    paracrop_poly <- st_sfc(poly, crs = 4326) %>% st_sf()
    pixel_list_test_poly <- lsat_get_pixel_centers(paracrop_poly, plot_map=FALSE, pixel_prefix="S")
    
    all_pixel_lists[[i]] <- pixel_list_test_poly
  }
  print(i)
}

pixels <- bind_rows(all_pixel_lists)

generate_strings <- function(n) {
  # Generate a sequence from 1 to n and paste "S_" before each element
  strings <- paste("S_", 1:n, sep = "")
  return(strings)
}

# Example usage
n <- 159  # Length of the sequence
strings <- generate_strings(n)

pixels$sample_id <- strings
```

```{r}
task_list <- lsat_export_ts(pixels, start_doy=1, end_doy=365)
```

```{r}
task_list <- lsat_export_ts(test_sf, start_doy=1, end_doy=365)
```

```{r}
# Create a vector of file names
file_names <- paste0("lsatTS_export_chunk_", 1:37, ".csv")

# Read each file and row-bind them together
lsat.dt <- do.call("rbind", lapply(file_names, fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=5, fig.height=5}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             doy.rng = 1:365, 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Function to exract sample points that came from the same polygon, will be used to help fit the mean evi values of a polygon rather than individual sample points. Will need to be updated, works for when there are 10 points sampled from each polygon
extract_group <- function(sample_id) {
  group_number <- as.numeric(sub("S_", "", sample_id))
  higher_order_group <- ((group_number - 1) %/% 20) + 1
  return(higher_order_group)
}

for (i in 1:nrow(lsat.dt)) {
  range <- c(1, n_pts_per_polygon[i])
  sample_ids <- range[1]:range[2]
  
}

extract_group <- function(sample_id, group_lengths) {
  group_numbers <- as.numeric(sub("S_", "", sample_id))
  cumulative_lengths <- c(0, cumsum(group_lengths))
  higher_order_group <- rep(NA, length(sample_id))
  for (i in 1:length(cumulative_lengths)) {
    if (i == 1) {
      higher_order_group[group_numbers <= cumulative_lengths[i]] <- i
    } else {
      higher_order_group[group_numbers > cumulative_lengths[i - 1] & group_numbers <= cumulative_lengths[i]] <- i
    }
  }
  return(higher_order_group)
}

means <- lsat.dt %>%
  mutate(group.id = extract_group(sample.id, n_pts_per_polygon))

means$group.id <- means$group.id - 1

grouped_means <- means %>%
  group_by(doy, year, group.id) %>%
  summarize(evi = mean(evi))

# grouped_means <- filter(grouped_means, year >= 2010)

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- updated_curve_fit_mean(grouped_means, si = 'evi', spar = .6)
```

```{r}
# lsat.dt <- filter(lsat.dt, year >= 2010)

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- fit_phenological_curve_no_points(lsat.dt, si = 'evi', spar = .6)

# Gives some summary stats on the phenological curve fits
lsat.gs.dt <- lsat_summarize_growing_seasons(lsat.pheno.dt, si = 'evi', min.frac.of.max = 0.75)
```

```{r}
#Creates boxplots for num of Obs vs % diff from max observed NDVI
lsat.gs.eval.dt <- lsat_evaluate_phenological_max(lsat.pheno.dt, si = 'evi', min.obs = 10, reps = 5, min.frac.of.max = 0.75, outdir = NA)
```

```{r}
# Create data for and plot histogram of relative change in greeness
lsat.trnds.dt <- lsat_calc_trend(lsat.gs.dt, si = 'evi.max', 1984:2024, sig = 0.1)
lsat_plot_trend_hist(lsat.trnds.dt, xlim=c(-21,21))
```

```{r}
# Create leaflet plot with trends imposed
colors.dt <- data.table(trend.cat=c("greening","no_trend","browning"),
                        trend.color=c("springgreen","white","orange"))

lsat.trnds.dt <- lsat.trnds.dt[colors.dt, on='trend.cat']

lsat.trnds.dt <- na.omit(lsat.trnds.dt)

lsat.trnds.sf <- st_as_sf(lsat.trnds.dt,
                          coords=c("longitude", "latitude"),
                          crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addPolygons(data = top_10_para,
              color = 'white',
              weight = 3) %>%
  addCircleMarkers(data=lsat.trnds.sf,
                   color='white',
                   weight=1,
                   opacity=0.9,
                   fillColor=~trend.color,
                   fillOpacity=0.5,
                   radius=~sqrt(abs(total.change.pcnt))*1) %>%
  addLegend('bottomright',
            colors=colors.dt$trend.color,
            labels=colors.dt$trend.cat,
            title='NDVImax trend',
            opacity=1)
```

Find the minimum evi value grouped by plantation

```{r}
min_evi_plantation <- means %>%
    group_by(group.id) %>%
    filter(evi == min(evi)) %>%
    select(group.id, min_evi = evi, year)
```

```{r}
min_evi_sample <- means %>%
    group_by(sample.id) %>%
    filter(evi == min(evi)) %>%
    select(sample.id, min_evi = evi, year)
```
