---
title: "Brazil AOI Landsat Application"
author: "Ryan DeStefano"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
#| message: false
library(sf)
library(dplyr)
library(purrr)
library(data.table)
library(stringr)
library(rgee)
library(lwgeom)
library(leaflet)
library(LandsatTS)
library(ggplot2)
```

```{r}
read_shapefile <- function(filepath, crs, make_valid = F) {
  
  # Read in the file
  file <- st_read(filepath)

  # Make it valid, gets rid of crossing polygon lines etc.
  file <- st_make_valid(file)
  
  # Transform to WGS84 (EPSG:4326)
  file <- st_transform(file, crs = crs)
  
  if (make_valid == T) {
    file <- file[st_is_valid(file),]
  }

}

# Read in the shapefile
paracrop <- read_shapefile("C:/Users/Ryman/OneDrive/Documents/Thesis/ParaCropShapefiles (Master)/ParaCropShapefiles/ParaCropShapefiles.shp", 4326)

# Plot the general location of the polygons, change the longitude and latitude as needed
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE)) %>%
  addMarkers(lat = -1.5, lng = -50.91394) %>%
  setView(lng = -50.91394, lat = -1.5, zoom = 2.5)
```

```{r}
para <- paracrop %>% 
  arrange(desc(Shape_Area)) %>% 
  mutate(estimated_points = Shape_Area / 0.00000006)

# Define the function to sample points from polygons and store in a tibble
sample_points_from_polygons <- function(df, n_pts_per_polygon) {
  # Create an empty tibble to store the sampled points
  test <- tibble(geometry = vector("list", sum(n_pts_per_polygon)),
                 sample_id = character(sum(n_pts_per_polygon)))

  # Initialize the sample ID counter
  sample_counter <- 1

  # Loop through each region to sample points
  for (i in 1:nrow(df)) {
    # Sample points from the i-th polygon
    pts <- st_sample(x = df[i, ], size = n_pts_per_polygon[i])
    
    # Convert the sampled points to their WKT representation
    point_wkt <- st_as_text(pts)
    
    # Determine the indices to store the sampled points
    start_index <- sample_counter
    end_index <- sample_counter + n_pts_per_polygon[i] - 1
    
    # Store the WKT representation of the sampled point geometries and sample IDs in the test tibble
    test$geometry[start_index:end_index] <- point_wkt
    test$sample_id[start_index:end_index] <- paste0('S_', start_index:end_index)
          
    # Increment the sample ID counter
    sample_counter <- end_index + 1
  }

  # Convert the test tibble to an sf object
  test_sf <- st_as_sf(test, wkt = "geometry", crs = 4326)
  
  return(test_sf)
}

n_pts_per_polygon <- num_sample_points(para)

test_sf <- sample_points_from_polygons(para, n_pts_per_polygon = n_pts_per_polygon)
```

```{r}
# Plot the sample points
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addCircleMarkers(data = test_sf, 
                   color = 'white',
                   opacity = 0.9,
                   fillColor = 'fuchsia',
                   fillOpacity = 0.75,
                   weight = 1,
                   radius = 5) %>%
  addPolygons(data = para,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

```{r}
# Export the picel measurments to google drive
task_list <- lsat_export_ts(test_sf, start_doy=1, end_doy=365)
```

```{r}
# Create a vector of file names
file_names <- paste0("lsatTS_export_chunk_", 1:37, ".csv")

# Read each file and row-bind them together
lsat.dt <- do.call("rbind", lapply(file_names, fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=5, fig.height=5}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             doy.rng = 1:365, 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Function to assign the group (essentially plantation) to each of the sample points
extract_group <- function(sample_id, group_lengths) {
    
    group_numbers <- as.numeric(sub("S_", "", sample_id))
    cumulative_lengths <- c(0, cumsum(group_lengths))
    higher_order_group <- rep(NA, length(sample_id))
    
    for (i in 1:length(cumulative_lengths)) {
        if (i == 1) {
            higher_order_group[group_numbers <= cumulative_lengths[i]] <- i
        } else {
            higher_order_group[group_numbers > cumulative_lengths[i - 1] & group_numbers <= cumulative_lengths[i]] <- i
        }
    }
    
    return(higher_order_group)
}

# Define the function to process the groups and calculate mean EVI values
calculate_grouped_means <- function(lsat_dt, n_pts_per_polygon) {
  # Assign the group IDs
  groups <- lsat_dt %>%
    mutate(group.id = extract_group(sample.id, n_pts_per_polygon))
  
  # Fix the magnitude of the group ID being off by one
  groups$group.id <- groups$group.id - 1
  
  # Find the mean EVI value for each group, day, and year combo
  grouped_means <- groups %>%
    group_by(doy, year, group.id) %>%
    summarize(evi = mean(evi, na.rm = TRUE), .groups = 'drop')
  
  return(grouped_means)
}

grouped_means <- calculate_grouped_means(lsat.dt, n_pts_per_polygon)

# Fit phenological models (cubic splines) to each plantation
lsat.pheno.dt <- updated_curve_fit_mean(grouped_means, si = 'evi', spar = .6, window.yrs = 3)
```

```{r}
# This fit is done for every pixel sampled instead of at the plantation level like the above
lsat.pheno.dt <- fit_phenological_curve_no_points(lsat.dt, si = 'evi', spar = .6)

# Gives some summary stats on the phenological curve fits
lsat.gs.dt <- lsat_summarize_growing_seasons(lsat.pheno.dt, si = 'evi', min.frac.of.max = 0.75)
```

```{r}
#Creates boxplots for num of Obs vs % diff from max observed NDVI
lsat.gs.eval.dt <- lsat_evaluate_phenological_max(lsat.pheno.dt, si = 'evi', min.obs = 10, reps = 5, min.frac.of.max = 0.75, outdir = NA)
```

```{r}
# Create data for and plot histogram of relative change in greeness
lsat.trnds.dt <- lsat_calc_trend(lsat.gs.dt, si = 'evi.max', 1984:2024, sig = 0.1)
lsat_plot_trend_hist(lsat.trnds.dt, xlim=c(-21,21))
```

```{r}
# Create leaflet plot with trends imposed
colors.dt <- data.table(trend.cat=c("greening","no_trend","browning"),
                        trend.color=c("springgreen","white","orange"))

lsat.trnds.dt <- lsat.trnds.dt[colors.dt, on='trend.cat']

lsat.trnds.dt <- na.omit(lsat.trnds.dt)

lsat.trnds.sf <- st_as_sf(lsat.trnds.dt,
                          coords=c("longitude", "latitude"),
                          crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addPolygons(data = top_10_para,
              color = 'white',
              weight = 3) %>%
  addCircleMarkers(data=lsat.trnds.sf,
                   color='white',
                   weight=1,
                   opacity=0.9,
                   fillColor=~trend.color,
                   fillOpacity=0.5,
                   radius=~sqrt(abs(total.change.pcnt))*1) %>%
  addLegend('bottomright',
            colors=colors.dt$trend.color,
            labels=colors.dt$trend.cat,
            title='EVImax trend',
            opacity=1)
```

```{r}
# Define the function to find the minimum EVI spline fit for each plantation
find_min_evi <- function(data) {
  data %>%
    group_by(group.id) %>%
    filter(spl.fit == min(spl.fit)) %>%
    filter(year == min(year)) %>%
    select(group.id, min_evi = spl.fit, year) %>%
    distinct(group.id, .keep_all = TRUE)
}

# Use the function on your dataset
min_evi <- find_min_evi(lsat.pheno.dt)
```

```{r}
# Assign a group.id to the original dataframe
para$group.id <- 1:nrow(para)

# Join min_evi and para by group.id
joined <- min_evi %>% 
  left_join(para, by = "group.id")

# COnvert to a dataframe
joined <- as.data.frame(joined)

# Convert your dataframe to a spatial object
sf_joined <- st_as_sf(joined)

# Define custom color palette with vibrant shades of red and green
color_palette <- colorNumeric(
  palette = c("#8B0000", "#FF6347", "#FFFFE0", "#ADFF2F", "#006400"),
  domain = 1984:2024
)

# Create leaflet map with legend
map <- leaflet(sf_joined) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(lng = -48, lat = -1.6, zoom = 8.2) %>%
  addPolygons(data = sf_joined, fillColor = ~color_palette(year), 
              color = "grey", weight = 1, opacity = 1, fillOpacity = 0.7,
              label = ~as.character(year)) %>%
  addLegend(position = "bottomright", 
            pal = color_palette, 
            values = ~year,
            title = "Year",
            opacity = 0.7,
            labFormat = labelFormat(suffix = "", digits = 0, big.mark = "")) %>% 
    addScaleBar(options = scaleBarOptions(imperial = FALSE))
            
map
```

```{r}
ggplot(joined, aes(x = year)) +
  geom_density(alpha = 0.5, fill = "skyblue") +
  labs(title = "Predicted Deforestation Year density Plot", x = "Deforestation Year", y = "Density") +
  theme_bw()
```

```{r}
# Initialize a new_year column
joined$new_year <- NA

# Bin the years based on their value
for (i in 1:nrow(joined)){
  
  if (joined$year[i] >= 1984 & joined$year[i] <= 1993){
    joined$new_year[i] = "1984-1993"
  }
  
  else if (joined$year[i] >= 1994 & joined$year[i] <= 2003){
    joined$new_year[i] = "1994-2003"
  }
  
  else if (joined$year[i] >= 2004 & joined$year[i] <= 2013){
    joined$new_year[i] = "2004-2013"
  }
  
  else if (joined$year[i] >= 2014){
    joined$new_year[i] = "2014+"
  }
}

# Calculate the count of rows for each unique year
year_counts <- joined %>%
  group_by(new_year) %>%
  summarize(row_count = n())

# Calculate the proportion of rows for each unique year
year_proportions <- year_counts %>%
  mutate(proportion = row_count / nrow(joined))

year_proportions
```

```{r}
translate_to_english <- function(portuguese_vector) {
    # English translations of the Portuguese words
    translations <- c("URBANO" = "Urban",
                      "DENDÊ" = "Palm Oil",
                      "FLORESTA_MADURA" = "Mature Forest",
                      "CACAU" = "Cocoa",
                      "VEGETAÇÃO_SECUNDÁRIA" = "Secondary Vegetation",
                      "PASTAGEM" = "Pasture",
                      "SILVICULTURA" = "Silviculture",
                      "AGRICULTURA_ANUAL" = "Annual Agriculture",
                      "CITRUS" = "Citrus",
                      "COCO" = "Coconut",
                      "AÇAÍ" = "Açaí",
                      "VIVEIRO_DE_MUDAS_DE_DENDÊ" = "Palm Seedling Nursery",
                      "FLORESTA_PRIMÁRIA" = "Primary Forest",
                      "PIMENTA_REINO" = "Black Pepper",
                      "SAF" = "Agroforestry System",
                      "PASTO_LIMPO" = "Clean Pasture",
                      "MAMÃO" = "Papaya",
                      "PASTO_SUJO" = "Dirty Pasture",
                      "MACAXEIRA" = "Cassava",
                      "CAFÉ" = "Coffee")

    # Translate the Portuguese words to English
    english_vector <- translations[portuguese_vector]
    
    return(english_vector)
}

para$crops <- sapply(strsplit(para$COBERTURA, "\\+"), function(x) x[1])

para$crops <- translate_to_english(para$crops)

crop <- joined %>% 
  left_join(para, by = "group.id")

crop_table <- as.data.frame(table(crop$crops))

crop <- crop %>% 
  filter(crops == "Açaí" | crops == "Palm Oil" | crops == "Cocoa")
```

```{r}
# Create the overlaid histograms
ggplot(crop, aes(x = year, fill = factor(crops))) +
  geom_density(alpha = 0.5) +
  labs(x = "Deforestation Year", y = "Density") +
  scale_fill_discrete(name = "Crop Type") +
  theme_bw()

```

```{r}
palm <- crop %>% 
  filter(crops == "Palm Oil")

ggplot(palm, aes(x = year, fill = factor(crops))) +
  geom_density(alpha = 0.5) +
  labs(x = "Deforestation Year", y = "Density") +
  scale_fill_manual(name = "Crop Type", values = c("Palm Oil" = "skyblue")) +
  ylim(0, 0.06) +
  theme_bw()

```

```{r}
cocoa <- crop %>% 
  filter(crops == "Cocoa")

ggplot(cocoa, aes(x = year, fill = factor(crops))) +
  geom_density(alpha = 0.5) +
  labs(x = "Deforestation Year", y = "Density") +
  scale_fill_manual(name = "Crop Type", values = c("Cocoa" = "skyblue")) +
  ylim(0, 0.06) +
  theme_bw()
```

```{r}
acai <- crop %>% 
  filter(crops == "Açaí")

ggplot(acai, aes(x = year, fill = factor(crops))) +
  geom_density(alpha = 0.5) +
  labs(x = "Deforestation Year", y = "Density") +
  scale_fill_manual(name = "Crop Type", values = c("Açaí" = "skyblue")) +
  ylim(0, 0.06) +
  theme_bw()
```
