---
title: "Peru AOI Landsat Application"
author: "Ryan DeStefano"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
#| message: false
library(sf)
library(dplyr)
library(purrr)
library(data.table)
library(stringr)
library(rgee)
library(lwgeom)
library(leaflet)
library(ggplot2)
library(LandsatTS)
```

```{r}
# Read in the shapefile
peru <- read_shapefile("04 Dataset_Ucayali_Palm_V2.geojson", 4326, make_valid = T)
```

```{r}
# Calculate the area of each plantation and sort descending by area
peru <- peru %>% 
  mutate(Area = st_area(peru$geometry)) %>% 
  arrange(desc(Area))

# Initialize the vector of number of points to take from the brazil application
provided_vector <- c(rep(20, 325), rep(19, 4), rep(18, 9), rep(17, 9), rep(16, 9), 
                     rep(15, 6), rep(14, 9), rep(13, 8), rep(12, 9), rep(11, 9), 
                     rep(10, 9), rep(9, 18), rep(8, 18), rep(7, 27), rep(6, 36), 
                     rep(5, 45), rep(4, 63), rep(3, 81), rep(2, 108), rep(1, 167))

# Desired length of new vector
desired_length <- 1281

# Calculate proportions
proportions <- table(provided_vector) / length(provided_vector)

# Calculate number of occurrences for each unique number
occurrences <- round(proportions * desired_length)

# Create the new vector
new_vector <- rep(as.numeric(names(occurrences)), times = occurrences)

# Trim the new vector to the desired length
new_vector <- new_vector[1:desired_length]

# Reverse the vector so the bigger plantations correspond to more sample points
new_vector <- rev(new_vector)

# Create a vector specifying the number of points to be sampled from each polygon
n_pts_per_polygon <- new_vector  # Example vector, replace with your desired values

# Sample the points and save them in a spatial object
test_sf <- sample_points_from_polygons(peru, n_pts_per_polygon = n_pts_per_polygon)

# Plot the polygons
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addCircleMarkers(data = test_sf, 
                   color = 'white',
                   opacity = 0.9,
                   fillColor = 'fuchsia',
                   fillOpacity = 0.75,
                   weight = 1,
                   radius = 5) %>%
  addPolygons(data = peru,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

```{r}
# Define your specific latitude and longitude
lat <- -8.179632
lon <- -74.95231

# Plot general Peru location
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE)) %>%
  addMarkers(lat = lat, lng = lon, popup = "Peru Plantations")  %>%
  setView(lng = lon, lat = lat, zoom = 2.5) %>% 
  addScaleBar(options = scaleBarOptions(imperial = FALSE))

```

```{r}
# Exort the points
task_list <- lsat_export_ts(test_sf, start_doy=1, end_doy=365)
```

```{r}
# Create a vector of file names
file_names <- paste0("lsatTS_export_chunk_", 1:50, ".csv")

# Read each file and row-bind them together
lsat.dt <- do.call("rbind", lapply(file_names, fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
# Display some general summary statistics
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=10, fig.height=10}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             doy.rng = 1:365, 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Calculate the mean EVI values foreach plantation for each point in time
grouped_means <- calculate_grouped_means(lsat.dt, n_pts_per_polygon)

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- updated_curve_fit_mean(grouped_means, si = 'evi', spar = .6, window.yrs = 3)
```

```{r}
# Fit phenological models (cubic splines) to each time series (Took out vi.min arg) for individual sample points instead of plantations
lsat.pheno.dt <- updated_curve_fit_no_points(lsat.dt, si = 'evi', spar = .6)

# Gives some summary stats on the phenological curve fits
lsat.gs.dt <- lsat_summarize_growing_seasons(lsat.pheno.dt, si = 'evi', min.frac.of.max = 0.75)
```

```{r}
#Creates boxplots for num of Obs vs % diff from max observed NDVI
lsat.gs.eval.dt <- lsat_evaluate_phenological_max(lsat.pheno.dt, si = 'evi', min.obs = 10, reps = 5, min.frac.of.max = 0.75, outdir = NA)
```

```{r}
# Create data for and plot histogram of relative change in greeness
lsat.trnds.dt <- lsat_calc_trend(lsat.gs.dt, si = 'evi.max', 1984:2024, sig = 0.1)
lsat_plot_trend_hist(lsat.trnds.dt, xlim=c(-21,21))
```

```{r}
# Define colors and trends data table
colors.dt <- data.table(trend.cat=c("greening","no_trend","browning"),
                        trend.color=c("springgreen","white","orange"))

# Merge trend colors with lsat trends data table
lsat.trnds.dt <- lsat.trnds.dt[colors.dt, on='trend.cat']

# Remove NA values from lsat trends data table
lsat.trnds.dt <- na.omit(lsat.trnds.dt)

# Convert lsat trends data table to simple features (sf) object
lsat.trnds.sf <- st_as_sf(lsat.trnds.dt,
                          coords=c("longitude", "latitude"),
                          crs=4326)

# Create Leaflet map object
leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%  # Add base layer with satellite imagery
  addPolylines(data=top_10_peru, color='white', weight=3) %>%  # Add polyline layer
  addCircleMarkers(data=lsat.trnds.sf,  # Add circle markers layer
                   color='white',  # Marker border color
                   weight=1,  # Marker border weight
                   opacity=0.9,  # Marker opacity
                   fillColor=~trend.color,  # Fill color based on trend color
                   fillOpacity=0.5,  # Fill opacity
                   radius=~sqrt(abs(total.change.pcnt))*1) %>%  # Marker radius based on total change percentage
  addLegend('bottomright',  # Add legend to bottom right corner
            colors=colors.dt$trend.color,  # Legend colors
            labels=colors.dt$trend.cat,  # Legend labels
            title='EVImax trend',  # Legend title
            opacity=1)  # Legend opacity
```

```{r}
# Find the point in time in which the minimum EVI value occurs for each plantation
# Saves the info in a dataframe which has each plantation with the year and value of EVI '
# in which the minimum was found
min_evi <- find_min_evi(lsat.pheno.dt)
```

```{r}
# Assign a group.id to the original dataframe
peru$group.id <- 1:nrow(peru)
 
# Join min_evi and peru by group.id
joined <- min_evi %>% 
  left_join(peru, by = "group.id")

# convert to dataframe
joined <- as.data.frame(joined)

# Normalize the year values between 0 and 1
joined <- joined %>% 
  mutate(norm_year = (year - min(year)) / (max(year) - min(year)))

# Convert your dataframe to a spatial object
sf_joined <- st_as_sf(joined)

# Define custom color palette with vibrant shades of red and green
color_palette <- colorNumeric(
  palette = c("#8B0000", "#FF6347", "#FFFFE0", "#ADFF2F", "#006400"),
  domain = 1984:2024
)

# Create leaflet map with legend
map <- leaflet(sf_joined) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(lng = -75, lat = -8.6, zoom = 8.2) %>%
  addPolygons(data = sf_joined, fillColor = ~color_palette(year), 
              color = "grey", weight = 1, opacity = 1, fillOpacity = 0.7) %>%
  addLegend(position = "bottomright", 
            pal = color_palette, 
            values = ~year,
            title = "Year",
            opacity = 0.7,
            labFormat = labelFormat(suffix = "", digits = 0, big.mark = "")) %>% 
    addScaleBar(options = scaleBarOptions(imperial = FALSE)) %>% 
    leafletOptions(zoomSnap = 0.05)

            
map
```

```{r}
# Create a density plot of the predicted deforestation years for all plantations
ggplot(joined, aes(x = year)) +
  geom_density(alpha = 0.5, fill = "skyblue") +
  labs(x = "Deforestation Year", y = "Density") +
  theme_bw()
```

```{r}
# Initialize a new_year column
joined$new_year <- NA

# Bin the years based on their value
for (i in 1:nrow(joined)){
  
  if (joined$year[i] >= 1984 & joined$year[i] <= 1993){
    joined$new_year[i] = "1984-1993"
  }
  
  else if (joined$year[i] >= 1994 & joined$year[i] <= 2003){
    joined$new_year[i] = "1994-2003"
  }
  
  else if (joined$year[i] >= 2004 & joined$year[i] <= 2013){
    joined$new_year[i] = "2004-2013"
  }
  
  else if (joined$year[i] >= 2014){
    joined$new_year[i] = "2014+"
  }
}

# Calculate the count of rows for each unique year
year_counts <- joined %>%
  group_by(new_year) %>%
  summarize(row_count = n())

# Calculate the proportion of rows for each unique year
year_proportions <- year_counts %>%
  mutate(proportion = row_count / nrow(joined))

year_proportions
```