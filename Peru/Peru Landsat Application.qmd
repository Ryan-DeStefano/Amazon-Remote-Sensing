---
title: "Peru AOI Landsat Application"
author: "Ryan DeStefano"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
library(sf)
library(dplyr)
library(purrr)
library(data.table)
library(stringr)
library(rgee)
library(lwgeom)
library(leaflet)
library(ggplot2)
library(LandsatTS)
```

```{r}
# Path to the GeoJSON file
geojson_file <- "04 Dataset_Ucayali_Palm_V2.geojson"

# Read the GeoJSON file
main <- st_read(geojson_file)

# Path to output shapefile (without file extension)
output_shapefile <- "04 Dataset_Ucayali_Palm_V2.shp"

# Write the spatial data to a shapefile
st_write(geojson_data, dsn = output_shapefile)
```

```{r}
set.seed(27)

main <- st_read(
  "04 Dataset_Ucayali_Palm_V2.geojson")

main <- st_make_valid(main)

# Check the current coordinate reference system
st_crs(main)

# Transform to WGS84 (EPSG:4326)
main <- st_transform(main, crs = 4326)
```

```{r}
main <- main[st_is_valid(main),]

# Preallocate the areas vector
areas <- numeric(nrow(main))

# Loop through each geometry
for (i in 1:nrow(main)) {
  area <- st_area(main$geometry[i])
  areas[i] <- area
}

main$AREA <- areas
```

```{r}
top_10_peru <- main 

top_10_peru$estimated_points <- top_10_peru$AREA / 2

# Provided vector
provided_vector <- c(rep(20, 325), rep(19, 4), rep(18, 9), rep(17, 9), rep(16, 9), 
                     rep(15, 6), rep(14, 9), rep(13, 8), rep(12, 9), rep(11, 9), 
                     rep(10, 9), rep(9, 18), rep(8, 18), rep(7, 27), rep(6, 36), 
                     rep(5, 45), rep(4, 63), rep(3, 81), rep(2, 108), rep(1, 167))

# Desired length of new vector
desired_length <- 1281

# Calculate proportions
proportions <- table(provided_vector) / length(provided_vector)

# Calculate number of occurrences for each unique number
occurrences <- round(proportions * desired_length)

# Create the new vector
new_vector <- rep(as.numeric(names(occurrences)), times = occurrences)

# Trim the new vector to the desired length
new_vector <- new_vector[1:desired_length]

new_vector <- rev(new_vector)

# Create a vector specifying the number of points to be sampled from each polygon
n_pts_per_polygon <- new_vector  # Example vector, replace with your desired values

# Initialize the tibble with the appropriate size
test <- tibble(geometry = vector("list", sum(n_pts_per_polygon)),
               sample_id = character(sum(n_pts_per_polygon)))

# Initialize the sample ID counter
sample_counter <- 1

# Loop through each region to sample points
for (i in 1:nrow(top_10_peru)) {
  # Sample points from the i-th polygon
  pts <- st_sample(x = top_10_peru[i, ], size = n_pts_per_polygon[i])
  
  # Convert the sampled points to their WKT representation
  point_wkt <- st_as_text(pts)
  
  # Determine the indices to store the sampled points
  start_index <- sample_counter
  end_index <- sample_counter + n_pts_per_polygon[i] - 1
  
  # Store the WKT representation of the sampled point geometries and sample IDs in the test tibble
  test$geometry[start_index:end_index] <- point_wkt
  test$sample_id[start_index:end_index] <- paste0('S_', start_index:end_index)
        
  # Increment the sample ID counter
  sample_counter <- end_index + 1
}

test_sf <- st_as_sf(test, wkt = "geometry", crs=4326)

rerun <- test_sf[11251:11500, ]

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addCircleMarkers(data = test_sf, 
                   color = 'white',
                   opacity = 0.9,
                   fillColor = 'fuchsia',
                   fillOpacity = 0.75,
                   weight = 1,
                   radius = 5) %>%
  addPolygons(data = top_10_peru,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

```{r}
task_list <- lsat_export_ts(rerun, start_doy=1, end_doy=365)
```

```{r}
# Create a vector of file names
file_names <- paste0("lsatTS_export_chunk_", 1:50, ".csv")

# Read each file and row-bind them together
lsat.dt <- do.call("rbind", lapply(file_names, fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=10, fig.height=10}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             doy.rng = 1:365, 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Function to exract sample points that came from the same polygon, will be used to help fit the mean evi values of a polygon rather than individual sample points. Will need to be updated, works for when there are 10 points sampled from each polygon
extract_group <- function(sample_id) {
  group_number <- as.numeric(sub("S_", "", sample_id))
  higher_order_group <- ((group_number - 1) %/% 20) + 1
  return(higher_order_group)
}

for (i in 1:nrow(lsat.dt)) {
  range <- c(1, n_pts_per_polygon[i])
  sample_ids <- range[1]:range[2]
  
}

extract_group <- function(sample_id, group_lengths) {
  group_numbers <- as.numeric(sub("S_", "", sample_id))
  cumulative_lengths <- c(0, cumsum(group_lengths))
  higher_order_group <- rep(NA, length(sample_id))
  for (i in 1:length(cumulative_lengths)) {
    if (i == 1) {
      higher_order_group[group_numbers <= cumulative_lengths[i]] <- i
    } else {
      higher_order_group[group_numbers > cumulative_lengths[i - 1] & group_numbers <= cumulative_lengths[i]] <- i
    }
  }
  return(higher_order_group)
}

means <- lsat.dt %>%
  mutate(group.id = extract_group(sample.id, n_pts_per_polygon))

means$group.id <- means$group.id - 1

grouped_means <- means %>%
  group_by(doy, year, group.id) %>%
  summarize(evi = mean(evi))

bruh <- filter(grouped_means, group.id == 2)

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- updated_curve_fit_mean(grouped_means, si = 'evi', spar = .6)
```

```{r}
bruh <- filter(lsat.dt, sample.id == "S_10")

# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- fit_phenological_curve_no_points(bruh, si = 'evi', spar = .6)

lsat.pheno.dt <- lsat_fit_phenological_curves(bruh, si = 'evi', spar = .6)

# Gives some summary stats on the phenological curve fits
lsat.gs.dt <- lsat_summarize_growing_seasons(lsat.pheno.dt, si = 'evi', min.frac.of.max = 0.75)
```

```{r}
#Creates boxplots for num of Obs vs % diff from max observed NDVI
lsat.gs.eval.dt <- lsat_evaluate_phenological_max(lsat.pheno.dt, si = 'evi', min.obs = 10, reps = 5, min.frac.of.max = 0.75, outdir = NA)
```

```{r}
# Create data for and plot histogram of relative change in greeness
lsat.trnds.dt <- lsat_calc_trend(lsat.gs.dt, si = 'evi.max', 1984:2024, sig = 0.1)
lsat_plot_trend_hist(lsat.trnds.dt, xlim=c(-21,21))
```

```{r}
# Create leaflet plot with trends imposed
colors.dt <- data.table(trend.cat=c("greening","no_trend","browning"),
                        trend.color=c("springgreen","white","orange"))

lsat.trnds.dt <- lsat.trnds.dt[colors.dt, on='trend.cat']

lsat.trnds.dt <- na.omit(lsat.trnds.dt)

lsat.trnds.sf <- st_as_sf(lsat.trnds.dt,
                          coords=c("longitude", "latitude"),
                          crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addPolylines(data=top_10_peru, color='white', weight=3) %>%
  addCircleMarkers(data=lsat.trnds.sf,
                   color='white',
                   weight=1,
                   opacity=0.9,
                   fillColor=~trend.color,
                   fillOpacity=0.5,
                   radius=~sqrt(abs(total.change.pcnt))*1) %>%
  addLegend('bottomright',
            colors=colors.dt$trend.color,
            labels=colors.dt$trend.cat,
            title='NDVImax trend',
            opacity=1)
```

This creates a dataframe that contains each of the plantations and the year that the minimum evi value was observed
```{r}
min_evi_plantation <- grouped_means %>%
    group_by(group.id) %>%
    filter(evi == min(evi)) %>%
    select(group.id, min_evi = evi, year)

min_evi_plantation <- min_evi_plantation %>%
    group_by(group.id) %>%
    filter(year == min(year)) %>%
    select(group.id, min_evi, year)
```

This creates a dataframe that contains each of the sample points and the year that he minimum evi value was observed.
```{r}
min_evi_sample <- means %>%
    group_by(sample.id) %>%
    filter(evi == min(evi)) %>%
    select(sample.id, min_evi = evi, year)
```

The following code tracks and plots the minimum evi year for each plantation based on the fitted curves and plots a choropleth map based on the year values
```{r}
test <- lsat.pheno.dt %>%
    group_by(group.id) %>%
    filter(spl.fit == min(spl.fit)) %>%
    select(group.id, min_evi = spl.fit, year)

test <- test %>% 
  group_by(group.id) %>% 
  filter(year == min(year)) %>% 
  select(group.id, min_evi, year) %>% 
  distinct(group.id, .keep_all = TRUE)
```

```{r}
top_10_peru$group.id <- 1:nrow(top_10_peru)

joined <- test %>% 
  left_join(top_10_peru, by = "group.id")

joined <- as.data.frame(joined)

# Normalize the year values between 0 and 1
joined <- joined %>% 
  mutate(norm_year = (year - min(year)) / (max(year) - min(year)))

# Convert your dataframe to a spatial object
sf_joined <- st_as_sf(joined)

# Define custom color palette with vibrant shades of red and green
color_palette <- colorNumeric(
  palette = c("#8B0000", "#FF6347", "#FFFFE0", "#ADFF2F", "#006400"),
  domain = 1984:2024
)

# Create leaflet map with legend
map <- leaflet(sf_joined) %>%
  addProviderTiles("CartoDB.Positron") %>%
  setView(lng = -75, lat = -8.6, zoom = 8.2) %>%
  addPolygons(data = sf_joined, fillColor = ~color_palette(year), 
              color = "grey", weight = 1, opacity = 1, fillOpacity = 0.7) %>%
  addLegend(position = "bottomright", 
            pal = color_palette, 
            values = ~year,
            title = "Year",
            opacity = 0.7,
            labFormat = labelFormat(suffix = "", digits = 0, big.mark = ""))
            
map
```

