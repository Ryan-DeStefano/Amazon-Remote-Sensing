---
title: "Peru AOI Landsat Application"
author: "Ryan DeStefano"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: source
execute: 
  cache: true
---

```{r}
library(sf)
library(dplyr)
library(purrr)
library(data.table)
library(stringr)
library(rgee)
library(lwgeom)
library(leaflet)
library(LandsatTS)
```

```{r}
# Path to the GeoJSON file
geojson_file <- "04 Dataset_Ucayali_Palm_V2.geojson"

# Read the GeoJSON file
main <- st_read(geojson_file)

# Path to output shapefile (without file extension)
output_shapefile <- "04 Dataset_Ucayali_Palm_V2.shp"

# Write the spatial data to a shapefile
st_write(geojson_data, dsn = output_shapefile)
```

```{r}
set.seed(27)

main <- st_read(
  "04 Dataset_Ucayali_Palm_V2.geojson")

main <- st_make_valid(main)

# Check the current coordinate reference system
st_crs(main)

# Transform to WGS84 (EPSG:4326)
main <- st_transform(main, crs = 4326)
```

```{r}
main <- main[st_is_valid(main),]

# Preallocate the areas vector
areas <- numeric(nrow(main))

# Loop through each geometry
for (i in 1:nrow(main)) {
  area <- st_area(main$geometry[i])
  areas[i] <- area
}

main$AREA <- areas
```

```{r}
top_10_peru <- main %>% 
  arrange(desc(AREA)) 

top_10_peru$estimated_points <- top_10_peru$AREA / 2

given_vector <- c(rep(20, 325), rep(19, 4), rep(18, 9), rep(17, 9), rep(16, 6), rep(15, 9), rep(14, 9), rep(13, 9), rep(12, 12), 
                  rep(11, 9), rep(10, 18), rep(9, 27), rep(8, 33), rep(7, 45), rep(6, 78), rep(5, 103), rep(4, 145), 
                  rep(3, 243), rep(2, 406), rep(1, 629))

# Calculate proportions
proportions <- table(given_vector) / length(given_vector)

# Calculate occurrences for each number in the new vector
occurrences <- round(proportions * 1288)

# Create the new vector
new_vector <- rep(as.numeric(names(occurrences)), occurrences)

new_vector <- c(new_vector, 1, 1, 1)

# Check the length of the new vector
length(new_vector)  # Output: 1288

# Create a vector specifying the number of points to be sampled from each polygon
n_pts_per_polygon <- new_vector  # Example vector, replace with your desired values

# Initialize the tibble with the appropriate size
test <- tibble(geometry = vector("list", sum(n_pts_per_polygon)),
               sample_id = character(sum(n_pts_per_polygon)))

# Initialize the sample ID counter
sample_counter <- 1

# Loop through each region to sample points
for (i in 1:nrow(top_10_peru)) {
  # Sample points from the i-th polygon
  pts <- st_sample(x = top_10_peru[i, ], size = n_pts_per_polygon[i])
  
  # Convert the sampled points to their WKT representation
  point_wkt <- st_as_text(pts)
  
  # Determine the indices to store the sampled points
  start_index <- sample_counter
  end_index <- sample_counter + n_pts_per_polygon[i] - 1
  
  # Store the WKT representation of the sampled point geometries and sample IDs in the test tibble
  test$geometry[start_index:end_index] <- point_wkt
  test$sample_id[start_index:end_index] <- paste0('S_', start_index:end_index)
        
  # Increment the sample ID counter
  sample_counter <- end_index + 1
}

test_sf <- st_as_sf(test, wkt = "geometry", crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addCircleMarkers(data = test_sf, 
                   color = 'white',
                   opacity = 0.9,
                   fillColor = 'fuchsia',
                   fillOpacity = 0.75,
                   weight = 1,
                   radius = 5) %>%
  addPolygons(data = top_10_peru,
              color = 'white',
              weight = 3) %>%
  addScaleBar(options = scaleBarOptions(imperial = FALSE))
```

```{r}
task_list <- lsat_export_ts(test_sf)
```

```{r}
lsat.dt <- do.call("rbind", lapply(c("Top_10_Peru_250.csv"), fread))
```

```{r}
# Format the data
lsat.dt <- lsat_format_data(lsat.dt)

# Only keep observations where there was clear sky etc
lsat.dt <- lsat_clean_data(lsat.dt, geom.max = 15, cloud.max = 80, sza.max = 60, filter.cfmask.snow = T, filter.cfmask.water = T, filter.jrc.water = T)
```

```{r}
data.summary.dt <- lsat_summarize_data(lsat.dt)
data.summary.dt
```

```{r, fig.width=10, fig.height=10}
# Retrieve NDVI values
lsat.dt <- lsat_calc_spectral_index(lsat.dt, si = 'evi')

# Cross-calibrate NDVI among sensors using an approach based on Random Forest machine learning
# Note: Need lots of observations for this to work
lsat.dt <- lsat_calibrate_rf(lsat.dt, 
                             band.or.si = 'evi', 
                             min.obs = 5, 
                             frac.train = 0.75, 
                             overwrite.col = T, 
                             write.output = F)
```

```{r}
# Fit phenological models (cubic splines) to each time series (Took out vi.min arg)
lsat.pheno.dt <- lsat_fit_phenological_curves(lsat.dt, si = 'evi', spar = .6)

# Gives some summary stats on the phenological curve fits
lsat.gs.dt <- lsat_summarize_growing_seasons(lsat.pheno.dt, si = 'evi', min.frac.of.max = 0.75)
lsat.gs.dt
```

```{r}
#Creates boxplots for num of Obs vs % diff from max observed NDVI
lsat.gs.eval.dt <- lsat_evaluate_phenological_max(lsat.pheno.dt, si = 'evi', min.obs = 10, reps = 5, min.frac.of.max = 0.75, outdir = NA)
```

```{r}
# Create data for and plot histogram of relative change in greeness
lsat.trnds.dt <- lsat_calc_trend(lsat.gs.dt, si = 'evi.max', 2000:2020, sig = 0.1)
lsat_plot_trend_hist(lsat.trnds.dt, xlim=c(-21,21))
```

```{r}
# Create leaflet plot with trends imposed
colors.dt <- data.table(trend.cat=c("greening","no_trend","browning"),
                        trend.color=c("springgreen","white","orange"))

lsat.trnds.dt <- lsat.trnds.dt[colors.dt, on='trend.cat']

lsat.trnds.dt <- na.omit(lsat.trnds.dt)

lsat.trnds.sf <- st_as_sf(lsat.trnds.dt,
                          coords=c("longitude", "latitude"),
                          crs=4326)

leaflet() %>%
  addProviderTiles('Esri.WorldImagery') %>%
  addPolylines(data=top_10_peru, color='white', weight=3) %>%
  addCircleMarkers(data=lsat.trnds.sf,
                   color='white',
                   weight=1,
                   opacity=0.9,
                   fillColor=~trend.color,
                   fillOpacity=0.5,
                   radius=~sqrt(abs(total.change.pcnt))*1) %>%
  addLegend('bottomright',
            colors=colors.dt$trend.color,
            labels=colors.dt$trend.cat,
            title='NDVImax trend',
            opacity=1)
```
